import pandas as pd
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from scipy.sparse import csr_matrix

# Load datasets
books = pd.read_csv('Books.csv')
ratings = pd.read_csv('Ratings.csv')
users = pd.read_csv('Users.csv')

ratings['User-ID'] = ratings['User-ID'].astype(str)

# CONTENT-BASED RECOMMENDATION
def content_based_recommendation(book_title, n=5):
    book_mask = books['Book-Title'].str.contains(book_title, case=False, na=False)
    return books[book_mask][['Book-Title', 'Book-Author']].head(n)

# COLLABORATIVE FILTERING (USER-BASED & ITEM-BASED)
def collaborative_filtering(user_id, n=5, method='user'):
    user_id = str(user_id)

    user_item = ratings.pivot_table(index='User-ID', columns='ISBN', values='Book-Rating').fillna(0)

    if user_id not in user_item.index:
        print(f"User {user_id} not found in dataset!")
        return pd.DataFrame(columns=['Book-Title', 'Book-Author'])

    sparse_matrix = csr_matrix(user_item.values)

    if method == 'user':
        user_sim = cosine_similarity(sparse_matrix, dense_output=False)
        user_idx = user_item.index.get_loc(user_id)

        sim_scores = list(enumerate(user_sim[user_idx].toarray().flatten()))
        sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)[1:min(100, len(sim_scores))]

        similar_users = [user_item.index[i[0]] for i in sim_scores]
        weighted_ratings = user_item.loc[similar_users].T.dot([i[1] for i in sim_scores])
        weighted_ratings /= sum(i[1] for i in sim_scores)

    else: 
        item_sim = cosine_similarity(sparse_matrix.T, dense_output=False)
        item_sim_df = pd.DataFrame(item_sim.toarray(), index=user_item.columns, columns=user_item.columns)

        user_ratings = user_item.loc[user_id].dropna()
        scores = item_sim_df[user_ratings.index].dot(user_ratings).div(item_sim_df[user_ratings.index].sum(axis=1))
        weighted_ratings = scores.sort_values(ascending=False)

    recommended_books = weighted_ratings.index[:n]
    return books[books['ISBN'].isin(recommended_books)][['Book-Title', 'Book-Author']]

# HYBRID RECOMMENDATION
def hybrid_recommendation(user_id, liked_book, n=5):
    cb_books = content_based_recommendation(liked_book, n)
    cf_books = collaborative_filtering(user_id, n)

    if cf_books.empty:
        return cb_books

    return pd.concat([cb_books, cf_books]).drop_duplicates().head(n)

# TEST RECOMMENDATION SYSTEM
user_id = '276733'
book_title = 'Timeline'

print("\n\n CONTENT-BASED (Books similar to '{}'):".format(book_title))
print(content_based_recommendation(book_title))

print("\n\n USER-BASED CF (Recommendations for user {}):".format(user_id))
print(collaborative_filtering(user_id, method='user'))

print("\n\n ITEM-BASED CF (Recommendations for user {}):".format(user_id))
print(collaborative_filtering(user_id, method='item'))

print("\n\n HYBRID (Recommendations for user {} who liked '{}'):".format(user_id, book_title))
print(hybrid_recommendation(user_id, book_title))






!pip install nltk
import nltk
nltk.download('punkt_tab') # changed line: download punkt_tab 
import pandas as pd
import matplotlib.pyplot as plt
from nltk import word_tokenize
from nltk.util import ngrams
from collections import Counter
import math

# Load books dataset
books = pd.read_csv('Books.csv', low_memory=False)

# Combine 'Book-Title' and 'Book-Author' to form the corpus
books['Text'] = books['Book-Title'].astype(str) + ' ' + books['Book-Author'].astype(str)
corpus = ' '.join(books['Text'].tolist()).lower()
tokens = word_tokenize(corpus)

# 1. Zipf's Law: Frequency vs Rank
def zipfs_law(tokens):
    freq_dist = Counter(tokens)
    sorted_freqs = sorted(freq_dist.values(), reverse=True)
    ranks = list(range(1, len(sorted_freqs) + 1))

    plt.figure(figsize=(8, 5))
    plt.loglog(ranks, sorted_freqs)
    plt.title("Zipf's Law")
    plt.xlabel("Rank")
    plt.ylabel("Frequency")
    plt.grid(True)
    plt.show()

# 2. Heap's Law: Vocabulary size vs Corpus size
def heaps_law(tokens):
    vocab_sizes = []
    token_counts = []

    vocab = set()
    for i, token in enumerate(tokens):
        vocab.add(token)
        if i % 100 == 0:
            token_counts.append(i)
            vocab_sizes.append(len(vocab))

    plt.figure(figsize=(8, 5))
    plt.plot(token_counts, vocab_sizes)
    plt.title("Heap's Law")
    plt.xlabel("Total Tokens")
    plt.ylabel("Vocabulary Size")
    plt.grid(True)
    plt.show()

# 3.N-gram Model (Unsmoothed)
def ngram_model(tokens, n=2):
    ngram_counts = Counter(ngrams(tokens, n))
    print(f"\nTop 10 {n}-grams:")
    for ng, count in ngram_counts.most_common(10):
        print(f"{' '.join(ng)}: {count}")

    return ngram_counts

# Run analyses
zipfs_law(tokens)
heaps_law(tokens)
ngram_model(tokens, n=2)  # You can change n to 1 (unigram), 2 (bigram), etc.
