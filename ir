#text classification
import pandas as pd
from sklearn.metrics import accuracy_score
from sklearn.naive_bayes import MultinomialNB
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer

def load_data_from_csv(file_path):
    df = pd.read_csv(file_path)
    texts = df['documents'].tolist()
    labels = df['outcome'].tolist()
    return texts, labels

def train_and_evaluate(texts, labels):
    X_train, X_test, y_train, y_test = train_test_split(texts, labels, test_size=0.25, random_state=42)
    vectorizer = CountVectorizer()
    X_train_counts = vectorizer.fit_transform(X_train)
    X_test_counts = vectorizer.transform(X_test)

    classifier = MultinomialNB()
    classifier.fit(X_train_counts, y_train)

    y_pred = classifier.predict(X_test_counts)
    accuracy = accuracy_score(y_test, y_pred)
    print(f"Accuracy: {accuracy:.2f}")

    return vectorizer, classifier

def classify_text(text, vectorizer, classifier):
    text_counts = vectorizer.transform([text])
    prediction = classifier.predict(text_counts)[0]
    return prediction

file_path = 'ca2.csv'
# file_path = 'data.txt'

# If reading from CSV
texts, labels = load_data_from_csv(file_path)

vectorizer, classifier = train_and_evaluate(texts, labels)

new_text = "I like using this product"
result = classify_text(new_text, vectorizer, classifier)
print(f"Text: '{new_text}' â†’ Prediction: {result}")





#probabilistic
from sklearn.feature_extraction.text import TfidfVectorizer
import pandas as pd
from numpy import dot
from numpy.linalg import norm
from sklearn.feature_extraction.text import CountVectorizer
import numpy as np

def load_data_from_csv(file_path):
    df = pd.read_csv(file_path)
    documents = df['documents'].tolist()
    return documents

documents = load_data_from_csv('documents.csv')
# Text Preprocessing - Simple tokenization using split
filtered_token_set = []
for doc in documents:
    tokens = doc.lower().split()
    filtered_tokens = [token for token in tokens if token.isalnum()]
    filtered_token_set.append(" ".join(filtered_tokens))

# TF-IDF Vectorization
tv = TfidfVectorizer()
document_matrix = tv.fit_transform(filtered_token_set)
td = pd.DataFrame(document_matrix.toarray(), columns=tv.get_feature_names_out())

# Query Processing
query = ["natural language processing"]
query_vector = tv.transform(query).toarray()[0]

# Cosine Similarity Calculation
cos_sim = []
for i in range(td.shape[0]):
    doc_vector = td.iloc[i].values
    similarity = dot(doc_vector, query_vector) / (norm(doc_vector) * norm(query_vector))
    cos_sim.append(similarity)

# Print Results
print("Cosine Similarity Scores:")
for idx, score in enumerate(cos_sim):
    print(f"Document {idx+1}: {score:.4f}")

# Phase 1: Select top k documents
k = 2
top_k_indices = sorted(range(len(cos_sim)), key=lambda i: cos_sim[i], reverse=True)[:k]

relevant_documents = [i+1 for i in top_k_indices]
non_relevant_documents = [i+1 for i in range(len(documents)) if i not in top_k_indices]
print("\nRelevant docs in phase 1:", relevant_documents)
print("Non-relevant docs in phase 1:", non_relevant_documents)

# Phase 2: Binary Independence Model
cv = CountVectorizer()
cv_ans = cv.fit_transform(filtered_token_set)
tdm = pd.DataFrame(cv_ans.toarray(), columns=cv.get_feature_names_out())
tdm_bool = tdm.astype(bool).astype(int)

print("\nBinary Document-Term Matrix:")
print(tdm_bool)

# Apply Binary Independence Model (BIM) Formula for Query Terms
N = len(documents)  # Total number of documents
S = len(relevant_documents)  # Number of relevant documents

bim_scores = {}

# Compute BIM score only for query terms
query_tokens = query[0].split()
print("\nQuery tokens:", query_tokens)

for term in query_tokens:
    if term in tdm.columns:
        n = tdm_bool[term].sum()  # Number of documents containing the term
        s = tdm_bool.iloc[top_k_indices][term].sum()  # Number of relevant documents containing the term
        # Apply BIM formula
        numerator = (S + 0.5) / (S - s + 0.5)
        denominator = (n - s + 0.5) / (N - S - n + s + 0.5)
        bim_score = np.log(numerator / denominator)
        bim_scores[term] = bim_score

# Print BIM Scores for Query Terms
print("\nBinary Independence Model (BIM) Scores for Query Terms:")
for term, score in bim_scores.items():
    print(f"{term}: {score:.4f}")

# Calculate final scores for each document
score_list = []
for j in range(tdm_bool.shape[0]):
    vec = tdm_bool.iloc[j]
    score = 0
    for term in query_tokens:
        if term in bim_scores:
            score += vec[term] * bim_scores[term]
    score_list.append(score)
print("\nFinal BIM scores:", score_list)
